# -*- coding: utf-8 -*-
"""msai-ai-hc-hw5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PXlm7wFU5sqSleG7umXkCROwYJ2sPP3L
"""

#@title Install Latest Version of Some Packages

# No additional installs required for this assignment

#@title Import Python Libraries & Some Other Setup

# Google Cloud stuff
from google.cloud import bigquery
from google.colab import auth, data_table
data_table.enable_dataframe_formatter()

# Pandas and data science stuff
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Modeling stuff
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc
from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay

#@title Provide Google Credentials to Colab Runtime

auth.authenticate_user()
print('Authenticated')

#@title Enter Google Cloud/BigQuery Project ID and initialize the client

project_id = 'msai-ai-hc-hw1' #@param{type:"string"}
bq_client = bigquery.Client(project = project_id)

#@title SQL Query the BQ DB for information

# Define the MIMIC III path
MIMIC_III_Clinical_Data = 'physionet-data.mimiciii_clinical'
MIMIC_III_Notes_Data = 'physionet-data.mimiciii_notes'

# Load ICU stays data from SQL and filter on LOS
sql_icu = f"""
SELECT ICUSTAY_ID, HADM_ID, SUBJECT_ID, INTIME, OUTTIME, LOS
FROM `{MIMIC_III_Clinical_Data}.icustays`
WHERE LOS IS NOT NULL AND LOS > 0
"""

# Create a pandas DataFrame object
df_icu = bq_client.query(sql_icu).to_dataframe()

# Verify our data
df_icu.info()
df_icu.head()
df_icu.describe()

# Print uniques
print(f"\nUnique ICU Stays: {df_icu['ICUSTAY_ID'].nunique()}")
print(f"Unique Hospital Admissions: {df_icu['HADM_ID'].nunique()}")
print(f"Unique Patients: {df_icu['SUBJECT_ID'].nunique()}")

# Convert admission and discharge times to datatime type
df_icu['INTIME'] = pd.to_datetime(df_icu['INTIME'])
df_icu['OUTTIME'] = pd.to_datetime(df_icu['OUTTIME'])

# Load patients data from SQL
sql_pat = f"SELECT SUBJECT_ID, GENDER, DOB FROM `{MIMIC_III_Clinical_Data}.patients`"

# Create a pandas DataFrame object
df_pat = bq_client.query(sql_pat).to_dataframe()

# Verify our data
df_pat.info()
df_pat.head()
df_pat.describe()

# Convert date of birth times to datatime type
df_pat['DOB'] = pd.to_datetime(df_pat['DOB'])

# Print uniques
print(f"\nUnique patients: {df_pat['SUBJECT_ID'].nunique()}")

# Load admissions data from SQL
sql_adm = f"""
SELECT SUBJECT_ID, HADM_ID, ADMITTIME, DISCHTIME, ADMISSION_TYPE, INSURANCE, LANGUAGE, RELIGION, MARITAL_STATUS, ETHNICITY, DIAGNOSIS
FROM `{MIMIC_III_Clinical_Data}.admissions`
"""

# Create a pandas DataFrame object
df_adm = bq_client.query(sql_adm).to_dataframe()

# Verify our data
df_adm.info()
df_adm.head()
df_adm.describe()

# Print uniques
print(f"\nUnique admissions: {df_adm['HADM_ID'].nunique()}")

# Convert admission and discharge times to datatime type
df_adm['ADMITTIME'] = pd.to_datetime(df_adm['ADMITTIME'])
df_adm['DISCHTIME'] = pd.to_datetime(df_adm['DISCHTIME'])

#@title SQL Query the BQ DB for information - part 2

# Load diagnoses data from SQL
sql_diag = f"""
SELECT SUBJECT_ID, HADM_ID, SEQ_NUM, ICD9_CODE
FROM `{MIMIC_III_Clinical_Data}.diagnoses_icd`
WHERE ICD9_CODE IS NOT NULL
"""

# Create a pandas DataFrame object
df_diag = bq_client.query(sql_diag).to_dataframe()

# Verify our data
df_diag.info()
df_diag.head()
df_diag.describe()

# Print uniques
print(f"\nUnique diagnoses: {df_diag['ICD9_CODE'].nunique()}")

# Load procedures data from SQL, we will use 224384 itemid because
# it corresponds better to what we need (see slides)
sql_proc = f"""
SELECT ICUSTAY_ID, ITEMID, STARTTIME, ENDTIME
FROM `{MIMIC_III_Clinical_Data}.procedureevents_mv`
WHERE ITEMID = 224385
"""

# Create a pandas DataFrame object
df_proc = bq_client.query(sql_proc).to_dataframe()

# Verify our data
df_proc.info()
df_proc.head()
df_proc.describe()

# Convert intubation times to datatime type
df_proc['STARTTIME'] = pd.to_datetime(df_proc['STARTTIME'])

# Print uniques
print(f"\nUnique intubation procedures: {len(df_proc)}")

#@title Data preprocessing and calculations

# First I want to only keep the first instance of the intubation for each
# patient. There is a chance that a pt gets int and extubated multiple times

df_first_intubation = df_proc.sort_values(by=['ICUSTAY_ID', 'STARTTIME']).drop_duplicates(subset=['ICUSTAY_ID'], keep='first')

# Copy the ICU dataframe so that pandas doesnt throw the SettingWithCopyWarnings warning
df_icu_target_var = df_icu[['ICUSTAY_ID', 'HADM_ID', 'SUBJECT_ID', 'INTIME']].copy()

# Merge the relevant dataframes, rename a column for clarity, then convert to datatime type
df_icu_target_var = df_icu_target_var.merge(df_first_intubation[['ICUSTAY_ID', 'STARTTIME']], on='ICUSTAY_ID', how='left')
df_icu_target_var.rename(columns={'STARTTIME': 'VENT_START_TIME'}, inplace=True)
df_icu_target_var['VENT_START_TIME'] = pd.to_datetime(df_icu_target_var['VENT_START_TIME'])

# Calculate the time difference between vent and admission, then convert to hours
df_icu_target_var['TIME_TO_VENT'] = df_icu_target_var['VENT_START_TIME'] - df_icu_target_var['INTIME']
df_icu_target_var['TIME_TO_VENT_IN_H'] = df_icu_target_var['TIME_TO_VENT'].dt.total_seconds() / 3600

# New column for our binary regression, anyone on vent under 24h gets a 1, otherwise 0
df_icu_target_var['VENT_UNDER_24H'] = 0
df_icu_target_var.loc[(df_icu_target_var['TIME_TO_VENT_IN_H'] > 0) & (df_icu_target_var['TIME_TO_VENT_IN_H'] <= 24), 'VENT_UNDER_24H'] = 1

# Verify our data
print(df_icu_target_var['VENT_UNDER_24H'].value_counts())
print(df_icu_target_var['VENT_UNDER_24H'].value_counts(normalize=True))

# Plot the distribution of our target variable of people on vent under 24h
plt.figure(figsize=(8, 6))
sns.countplot(x='VENT_UNDER_24H', data=df_icu_target_var)
plt.title('Distribution of target variable\nIntubation under 24h')
plt.xlabel('Intubation < 24h (0=No, 1=Yes)')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

#@title Feature engineering

# For filtering our labs and vitals we need relevant ICUSTAY_ID and HADM_ID
rel_icu_stays = df_icu_target_var['ICUSTAY_ID'].unique()
rel_hadms = df_icu_target_var['HADM_ID'].unique()

# Convert those to lists
rel_icu_stays = rel_icu_stays.tolist()
rel_hadms = rel_hadms.tolist()

# Convert ICUSTAY into a SQL list
rel_icu_stays = ', '.join(map(str, rel_icu_stays))

# set the timing window for the labs and vitals
feature_window_in_hours = 6

# SQL query the vitals
sql_vitals = f"""
WITH rel_icu_stays AS (
    SELECT ICUSTAY_ID, INTIME
    FROM `{MIMIC_III_Clinical_Data}.icustays`
    WHERE ICUSTAY_ID IN ({rel_icu_stays})
)
SELECT ch.ICUSTAY_ID, ch.ITEMID, ch.CHARTTIME, ch.VALUENUM
FROM `{MIMIC_III_Clinical_Data}.chartevents` AS ch
JOIN rel_icu_stays icu ON ch.ICUSTAY_ID = icu.ICUSTAY_ID
WHERE ch.ITEMID IN (220045, 220210, 220277, 223761, 220179, 223835)
  AND ch.CHARTTIME BETWEEN icu.INTIME AND TIMESTAMP_ADD(icu.INTIME, INTERVAL 6 HOUR)
"""

df_vitals = bq_client.query(sql_vitals).to_dataframe()

# Verify our data
df_vitals.info()
df_vitals.head()
df_vitals.describe()

##
# Aggregate and pivot the data
##

# Convert charting times to datatime type
df_vitals['CHARTTIME'] = pd.to_datetime(df_vitals['CHARTTIME'])

# Aggregate and pivot - had GPT help here
vitals_agg = df_vitals.groupby(['ICUSTAY_ID', 'ITEMID'])['VALUENUM'].agg(['mean', 'min', 'max']).reset_index()
vitals_features = vitals_agg.pivot_table(
  index='ICUSTAY_ID', columns='ITEMID', values=['mean', 'min', 'max']
  )
vitals_features.columns = [f'vital_{stat}_{itemid}' for stat, itemid in vitals_features.columns]
vitals_features.reset_index(inplace=True)

# print(vitals_features)

# vitals_features.dropna(inplace=True)

# print(vitals_features)

#@title Feature engineering - part 2

# SQL query the labs
sql_labs = f"""
WITH rel_icu_stays AS (
    SELECT ICUSTAY_ID, HADM_ID, INTIME
    FROM `{MIMIC_III_Clinical_Data}.icustays`
    WHERE ICUSTAY_ID IN ({rel_icu_stays})
)
SELECT icu.ICUSTAY_ID, lab.ITEMID, lab.CHARTTIME, lab.VALUENUM
FROM `{MIMIC_III_Clinical_Data}.labevents` AS lab
JOIN rel_icu_stays icu ON lab.HADM_ID = icu.HADM_ID
WHERE lab.ITEMID IN (50820, 50821, 50818, 50817, 50803, 51301)
    AND lab.CHARTTIME BETWEEN icu.INTIME AND TIMESTAMP_ADD(icu.INTIME, INTERVAL 6 HOUR)
"""

# Create a pandas DataFrame object
df_labs = bq_client.query(sql_labs).to_dataframe()

# Verify our data
df_labs.info()
df_labs.head()
df_labs.describe()

##
# Aggregate and pivot the data
##

# Convert charting times to datatime type
df_labs['CHARTTIME'] = pd.to_datetime(df_labs['CHARTTIME'])

# Sort, aggregate and pivot - as per example above
# We need to sort here because were interested in first measurement
# as well as min and max, which we did not in vitals
df_labs = df_labs.sort_values(by=['ICUSTAY_ID', 'ITEMID', 'CHARTTIME'])
labs_agg = df_labs.groupby(['ICUSTAY_ID', 'ITEMID'])['VALUENUM'].agg(['first', 'min', 'max']).reset_index()
labs_features = labs_agg.pivot_table(
  index='ICUSTAY_ID', columns='ITEMID', values=['first', 'min', 'max']
  )
labs_features.columns = [f'lab_{stat}_{itemid}' for stat, itemid in labs_features.columns]
labs_features.reset_index(inplace=True)
# labs_features.dropna(inplace=True)

print(labs_features)

#@title Merge features and target + final clean up

# Merge target with vitals
df_merged_features = df_icu_target_var.merge(vitals_features, on='ICUSTAY_ID', how='left')

# Merge with labs
df_merged_features = df_merged_features.merge(labs_features, on='ICUSTAY_ID', how='left')

# Verify our data
df_merged_features.info()
df_merged_features.head()
df_merged_features.describe()
df_merged_features.shape

# Drop columns we wont need
df_merged_features.drop(columns=['HADM_ID', 'SUBJECT_ID', 'INTIME','ICUSTAY_ID'], inplace=True, errors='ignore')
df_merged_features.info()
df_merged_features.head()
df_merged_features.describe()
df_merged_features.shape

#@title Model set up

# Separate target from features
X = df_merged_features.drop(columns=['VENT_UNDER_24H', 'VENT_START_TIME', 'TIME_TO_VENT', 'TIME_TO_VENT_IN_H'])
y = df_merged_features['VENT_UNDER_24H']

# Data split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
print(y_train.value_counts(normalize=True))
print(y_test.value_counts(normalize=True))

# Set up a pipeline, because different vitals and different labs have a
# differentf scale and we dont want it to over power certain features

num_pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                               ('scale', StandardScaler())])

#@title Model training

# We gonna traing two model: Logistic Regression and Random Forest
models = {
    "LR": LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced', solver='liblinear'),
    "RF": RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100, n_jobs=-1)
}

# Set up dictionaries to keep pipelines and results
pipelines = {}
results = {}

# Model training and evaluation
for acronym, model in models.items():
  pipeline = Pipeline(steps=[('preprocessor', num_pipeline), ('classifier', model)])

  pipeline.fit(X_train, y_train)
  pipelines[acronym] = pipeline
  y_pred = pipeline.predict(X_test)
  y_pred_proba = pipeline.predict_proba(X_test)[:, 1]

  accuracy = accuracy_score(y_test, y_pred)
  precision = precision_score(y_test, y_pred)
  recall = recall_score(y_test, y_pred)
  roc_auc = roc_auc_score(y_test, y_pred_proba)
  f1 = f1_score(y_test, y_pred)
  prec, rec, _ = precision_recall_curve(y_test, y_pred_proba)
  auc_pr = auc(rec, prec)


# Store results in the dictionary
  results[acronym] = {
      'accuracy': accuracy,
      'precision': precision,
      'recall': recall,
      'roc_auc': roc_auc,
      'f1': f1,
      'auc_pr': auc_pr,
      'report': classification_report(y_test, y_pred, zero_division=0),
      'y_pred': y_pred,
      'y_pred_proba': y_pred_proba
  }


# Display results for verification
  print(f"{acronym} Results:")
  print(f"Accuracy: {accuracy:.4f}")
  print(f"Precision: {precision:.4f}")
  print(f"Recall: {recall:.4f}")
  print(f"ROC AUC: {roc_auc:.4f}")
  print(f"F1 Score: {f1:.4f}")
  print(f"AUC PR: {auc_pr:.4f}")
  print(f"Classification Report:\n{classification_report(y_test, y_pred)}")
  print("\n")

#@title Evaluation, plotting feature importance

fig_curve, axes = plt.subplots(1, 2, figsize=(14, 6))

# Display results and plot
for acronym, result in results.items():
  print(f"{acronym} Results:")
  print(f"Accuracy: {result['accuracy']:.4f}")
  print(f"Precision: {result['precision']:.4f}")
  print(f"Recall: {result['recall']:.4f}")
  print(f"ROC AUC: {result['roc_auc']:.4f}")
  print(f"F1 Score: {result['f1']:.4f}")
  print(f"AUC PR: {result['auc_pr']:.4f}")
  print(f"Classification Report:\n{result['report']}")
  print("\n")

# Plot confusion matrix
  cm = confusion_matrix(y_test, result['y_pred'])
  fig_cm, ax_cm = plt.subplots(figsize=(5, 4))
  # plt.figure(figsize=(5,4))
  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
              xticklabels=['Predicted no vent', 'Predicted vent'],
              yticklabels=['Actual no vent', 'Actual vent'], ax=ax_cm)
  ax_cm.set_title(f'{acronym} Confusion Matrix')
  ax_cm.set_xlabel('Predicted')
  ax_cm.set_ylabel('Actual')
  fig_cm.tight_layout()
  plt.show()

# Calculate ROC curve
  RocCurveDisplay.from_predictions(y_test, result['y_pred_proba'], name=acronym, ax=axes[0])

# Calculate PR curve
  PrecisionRecallDisplay.from_predictions(y_test, result['y_pred_proba'], name=acronym, ax=axes[1])

# Comparison plots
axes[0].set_title('ROC Curves')
axes[0].plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)
axes[0].legend()
positive_prevalence = y_test.mean()
axes[1].set_title('PR Curves')
axes[1].axhline(positive_prevalence, linestyle='--', lw=2, color='r', label=f'Chance ({positive_prevalence:.2f})', alpha=.8)
# axes[1].plot([0, 1], [0.5, 0.5], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)
axes[1].legend()

fig_curve.tight_layout()
plt.show()

# Calculate feature importance for Random Forest
rf_pipeline = pipelines['RF']
rf_model = rf_pipeline.named_steps['classifier']
feature_names = X_train.columns.tolist()
importances = rf_model.feature_importances_

feature_importance = pd.DataFrame({'Importance': importances}, index=feature_names)
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

print(feature_importance.head(10))

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance.Importance.head(10), y=feature_importance.index[:10], palette='viridis')
plt.title('Random Forest Feature Importance')
plt.xlabel('Importance')
plt.ylabel('Features')
plt.tight_layout()
plt.show()